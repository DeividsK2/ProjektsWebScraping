Šī uzdevuma mērķis ir izmantot webscraping, lai automatizētu kaut kādu aspektu no dzīves. Es izvēlejos pielietot webscraping automatizēt ziņas iegūšanu no savas vietējām ziņām divu iemeslu dēļ:
Pirmkārt, tās ir vietējās ziņas, tāpēc man tās ir svarīgākas, un, ja es vēlos uzzināt par valsts mēroga ziņām, es skatītos TV vai izmantotu citas ziņu portālus.
Otrkārt, ziņas ir svarīgas, un es uzskatu, ka tās ir ikdienas nepieciešamība. Izmantojot tīmekļa datu ieguvi, to varētu automatizēt un potenciāli filtrēt nevēlamas ziņas vai izveidot konspektus, lai ātrāk iegūtu informāciju. To varētu izmantot arī citām lietām: piemēram, cīņai pret plaģiātu, jo, ja ziņas teksts ir izmantots vairākās ziņu vietnēs, varētu salīdzināt informāciju, iespējams, ar mākslīgā intelekta palīdzību, lai redzētu, vai kaut kas nav nokopēts. Līdzīgi to varētu izmantot, lai cīnītos pret viltus ziņām. Ja apkopojat informāciju no vairākām vietnēm, varētu salīdzināt rakstus par vienu un to pašu tēmu. Ja visi raksti ir vienādi, tad, visticamāk, ir patiesi. Ja nesakrīt, tad vajadzētu pārbaudīt, kas atšķirās un kura īstenība ir patiesība. Tas ir svarīgi, jo boti facebook vai youtube komentāros ir izplatīti jau gadiem ilgi, un, mākslīgajam intelektam kļūstot labākam, viltus ziņas varētu izplatīties arvien ātrāk, mākslīgais intelekts arī varētu izveidot viltotas ziņas vietnes, kas varētu apmānīt cilvēkus un mainīt viņu pasaules uzskatu.

Šajā darbā es izmantoju request un beautifulsoap, jo bez tiem uzdevums būtu diezgan grūts vai pat neiespējams. Tos izmanto, lai iegūtu informāciju no tīmekļa vietnēm, kas ir svarīgi tīmekļa datu ieguvei. Bez tiem nevarētu izmantot url = funkciju, tāpēc informācijas iegūšana būtu grūtāka.
Kad viss ir importēts un ir ielikts  URL, kods var darboties.
Tīmekļa vietne ir HTML formātā, tāpēc es nolēmu izmantot šīs zināšanas, lai pildītu projektu. BeautifulSoup izmanto html.parser, jo mājaslapa  ir rakstīta HTML, un tas nozīmē, ka mēs varam  izmantot HTML zināšanas. Pēc tam, atkal izmantojot soup, lai atrastu visus div elementus un precīzāk, kur atrodas klase = 'post', jo tie ir tie raksti, kuri ir vajadzīgi uzdevumam. 
Pēc tam tiem izmantots for cikls. Tas ir tāpēc, ka mums ir nepieciešami vairāki raksti, nevis tikai viens, kā arī tiks veikti vairākas darbības un ar ciklu būtu daudz vienkāršāk. Atkal kods meklē konkrētas HTML elementus. Tiek meklēts <h2> un klasi "entry-title". kas ir raksta tituls vai nosaukums, un tas pats notiek ar pārējam HTML elementiem,  a un href. href ir raksta URL, kas būs nepieciešams, lai iegūtu tā saturu.

Pēc tam atkal tiek izmantots soup, bet šoreiz, lai iegūtu informāciju no katra raksta, jo katrs ir sava lapa. Šoreiz kods meklē p, kas satur raksta saturu. Izmantojot ciklu, tas printē  raksta tekstu un atkārto to ar katru rakstu.
 Tiek izmantots filtrs, tas nav gluži nepieciešams, bet konkrētajā mājaslapā, tika printēts tas, ko nevajag, kas bija paslēpts zem dažam pogām. Šis filtrs varētu būt noderīgs, ja webscraping izmanto uz citām mājaslapām, vai arī varētu izmantot, lai konkrēti printētu filtrētus vārdus, piemēram, cik reizi tiek atkārtots konkrēts vārds? Vai arī atrast informāciju par konkrētu lietu.
